<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Offer Response Analysis</title>
    <style>
      body {
        font-family: Arial, sans-serif;
        line-height: 1.6;
        margin: 20px;
        color: rgb(178, 226, 210);
        background-color: rgb(14, 32, 26);
      }
      h1 {
        text-align: center;
        color: rgb(180, 230, 230);
      }

      h2 {
        color: rgb(180, 230, 230);
      }
      a {
        color: rgb(230, 180, 230);
      }

      p {
        display: block;
        margin-block-start: 1em;
        margin-block-end: 1em;
        margin-inline-start: 0px;
        margin-inline-end: 0px;
        unicode-bidi: isolate;
      }

      .container {
        max-width: 800px;
        margin: auto;
      }

      img {
        max-width: 100%;
        height: auto;
        display: block;
        margin: 20px auto;
      }

      .quote {
        font-style: italic;
        border-left: 2px solid #007bff;
        padding-left: 10px;
        margin: 20px 0;
      }

      .summary {
        font-weight: bold;
      }
    </style>
  </head>

  <body>
    <div class="container">
      <h1>Starbucks Analysis</h1>
      <img src="green_cafe.jpg" alt="cafe" />
      <p>
        Starbucks has been using various marketing offers, such as Buy One Get
        One (BOGO), Discount, and Informational campaigns, to engage their
        customers. The dataset used for this analysis contains user interaction
        events with these offers, including when an offer was received, viewed,
        or completed. The goal of this analysis is to understand which
        demographics and channels are most effective in driving offer
        completions and to provide insights for optimizing future marketing
        strategies.
      </p>

      <p>
        This data set contains simulated data that mimics customer behavior on
        the Starbucks rewards mobile app. Every few days, Starbucks sends out
        offers to users of the app, which can range from simple advertisements
        for drinks to more substantial offers like discounts or Buy One Get One
        Free (BOGO) promotions. Some users might receive offers regularly, while
        others may not receive any during certain weeks. Not all users receive
        the same offer, which presents a unique challenge for Starbucks.
      </p>

      <p>
        The task is to determine which demographic groups respond best to each
        type of offer and how Starbucks can optimize the delivery of these
        offers to maximize user engagement. The data set represents a simplified
        version of the real Starbucks app, focusing on a single product, while
        the actual app has a wide variety of offerings.
      </p>

      <p>
        Each offer has a validity period, which defines how long the customer
        can take advantage of it. For example, a BOGO offer may be valid for
        five days. Even informational offers have a validity period, allowing
        customers to be influenced by the information for a set number of days
        after receiving the offer.
      </p>

      <p>
        The data includes transactional records, indicating the amount spent by
        users and the timing of each transaction, along with interactions with
        offers—such as when an offer was received, viewed, or completed. It’s
        important to note that users may make purchases without ever receiving
        or viewing an offer. Similarly, a user may receive an offer but not open
        it, yet still make a qualifying purchase during the offer period.
      </p>

      <p>
        The primary challenge is to combine transactional, demographic, and
        offer data to determine which groups of people are most responsive to
        each type of offer and how to best present each offer. By understanding
        this, Starbucks can more effectively target different demographic groups
        through specific channels, ultimately optimizing marketing efforts and
        enhancing customer satisfaction.
      </p>
      <h2>Datasets</h2>
      <p>
        The analysis is based on three main datasets: <strong>Profile</strong>,
        <strong>Portfolio</strong>, and <strong>Transcript</strong>. Each
        dataset plays a crucial role in understanding user behavior, their
        response to offers, and the effectiveness of different types of
        marketing strategies. Below is a detailed description of each dataset:
      </p>
      <h3>Profile Dataset</h3>
      <p>
        The <strong>Profile</strong> dataset contains demographic information
        about users participating in the rewards program. It includes data for
        17,000 users across five fields:
      </p>
      <ul>
        <li>
          <strong>Gender</strong>: This variable is categorical and can take
          values such as 'M', 'F', 'O', or <code>null</code> if the information
          is missing. Understanding user gender helps in identifying
          gender-specific preferences and tailoring targeted offers accordingly.
        </li>
        <li>
          <strong>Age</strong>: A numerical value representing the user's age.
          Missing values are encoded as <code>118</code>. Age is a significant
          factor for understanding generational behavior patterns in offer
          response.
        </li>
        <li>
          <strong>ID</strong>: A unique identifier for each user, represented as
          a hashed string. This ID helps in linking users across the datasets.
        </li>
        <li>
          <strong>Became Member On</strong>: The date when the user joined the
          rewards program, given in <code>YYYYMMDD</code> format. This variable
          is helpful to understand how user engagement changes over time,
          depending on membership duration.
        </li>
        <li>
          <strong>Income</strong>: The annual income of the user, represented as
          a numeric value. Income is an important factor in determining which
          types of offers resonate better with different socioeconomic segments.
        </li>
      </ul>
      <h3>Portfolio Dataset</h3>
      <p>
        The <strong>Portfolio</strong> dataset includes details of all the
        offers sent out during the 30-day test period. It contains data for 10
        different offers across six fields:
      </p>
      <ul>
        <li>
          <strong>Reward</strong>: The monetary amount awarded if the user
          completes the offer's conditions.
        </li>
        <li>
          <strong>Channels</strong>: A list indicating the different
          communication channels used to deliver the offer. These can include
          <code>web</code>, <code>email</code>, <code>mobile</code>, and
          <code>social</code>. Understanding which channels are most effective
          is essential for optimizing marketing strategies.
        </li>
        <li>
          <strong>Difficulty</strong>: The monetary amount the user is required
          to spend to receive the reward. This is crucial for assessing user
          behavior under varying spending thresholds.
        </li>
        <li>
          <strong>Duration</strong>: The number of days the offer remains valid.
          This helps in analyzing the influence of offer validity period on user
          engagement.
        </li>
        <li>
          <strong>Offer Type</strong>: The type of offer, which could be
          <code>bogo</code>, <code>discount</code>, or
          <code>informational</code>. This is a critical variable for comparing
          which types of offers work best for different demographics.
        </li>
        <li>
          <strong>ID</strong>: A unique identifier for each offer, represented
          as a hashed string. This ID allows linking offer data with user
          interactions recorded in the transcript dataset.
        </li>
      </ul>
      <h3>Transcript Dataset</h3>
      <p>
        The <strong>Transcript</strong> dataset provides a detailed log of all
        events related to user interactions with the offers. It contains 306,648
        records across four main fields:
      </p>
      <ul>
        <li>
          <strong>Person</strong>: A unique identifier for each user,
          represented as a hashed string. This allows linking with demographic
          data from the profile dataset.
        </li>
        <li>
          <strong>Event</strong>: A string indicating the type of
          interaction—such as <code>offer received</code>,
          <code>offer viewed</code>, <code>transaction</code>, or
          <code>offer completed</code>. This field helps track the complete
          journey of an offer and understand the conversion funnel.
        </li>
        <li>
          <strong>Value</strong>: A dictionary that holds various values
          depending on the event type. It may contain <code>amount</code> (for
          transaction events) or <code>reward</code> (for completed offers).
          This field is instrumental in tracking user spending and reward
          earnings.
        </li>
        <li>
          <strong>Offer ID</strong>: The unique identifier for the offer
          involved in the interaction. It allows connecting specific events with
          the corresponding offer in the portfolio dataset.
        </li>
        <li>
          <strong>Amount</strong>: The amount spent during a transaction,
          applicable to <code>transaction</code> events. This helps in
          quantifying user spending behavior.
        </li>
        <li>
          <strong>Reward</strong>: The reward received upon completing an offer.
          This field is key to understanding how users respond to incentives.
        </li>
        <li>
          <strong>Time</strong>: A numeric value indicating the number of hours
          since the start of the test. It provides a timeline for each user’s
          interactions, helping to understand the sequence of events.
        </li>
      </ul>
      <h3>Significance of Key Variables</h3>
      <p>
        The <strong>Profile</strong> dataset gives a deep insight into user
        demographics, which are crucial for understanding which groups are more
        responsive to each type of offer. For example, variables like
        <em>age</em> and <em>income</em> help segment users, enabling the
        identification of which age groups or income brackets respond best to
        specific offers.
      </p>
      <p>
        The <strong>Portfolio</strong> dataset provides information about the
        characteristics of each offer, such as the type, reward, and
        communication channels used. This information is essential to analyze
        the performance of each offer and to understand which type of offers are
        more effective and through which channels.
      </p>
      <p>
        The <strong>Transcript</strong> dataset logs the entire user interaction
        with each offer, allowing for a complete analysis of the user journey.
        The <em>event</em> field is particularly important for understanding how
        many users viewed an offer after receiving it and eventually completed
        it. This dataset also helps quantify spending behavior (<em>amount</em>
        field) and reward earnings (<em>reward</em> field).
      </p>
      <p>
        By combining these datasets, we gain a holistic view of user
        behavior—from their demographic background to how they interact with
        offers. This integrated analysis enables us to answer critical questions
        regarding offer effectiveness, channel performance, and demographic
        preferences, ultimately guiding the development of a more targeted and
        effective marketing strategy for Starbucks.
      </p>

      <h2>Strategy for Solving the Problem</h2>
      <p>
        To tackle the problem of identifying which groups of people are most
        responsive to each type of offer and how best to present these offers,
        we employed a structured approach consisting of various stages, from
        project setup to modeling and evaluation. The methodology involved
        setting up a clear project structure, performing extensive exploratory
        data analysis (EDA), building a preprocessing pipeline, and finally
        applying machine learning techniques to create predictive models.
      </p>

      <img src="user.jpg" alt="typical user" />
      <h3>Overall Approach and Methodology</h3>
      <p>
        The overall approach began by organizing the project in a modular
        manner:
      </p>
      <ul>
        <li>
          <strong>Project Setup and Organization</strong>: The project was
          divided into different files and folders:
        </li>
        <ul>
          <li><code>data/</code>: Store the provided JSON files.</li>
          <li>
            <code>notebooks/</code>: Jupyter notebooks split by major phases,
            including EDA, preprocessing, and modeling.
          </li>
          <li>
            <code>src/</code>: Scripts for loading, preprocessing, and modeling
            functions.
          </li>
          <li>
            <code>README.md</code>: Outlines the purpose, approach, and setup
            instructions.
          </li>
          <li><code>requirements.txt</code>: For project dependencies.</li>
        </ul>
        <li>
          <strong>Initial Steps</strong>: Load the JSON files and conduct a
          preliminary overview to understand the basic structure, relationships,
          and any missing data in the datasets. This phase also involved setting
          up a notebook template with sections for project definition, analysis,
          methodology, and results.
        </li>
        <li>
          <strong>Project Definition</strong>: Formulated a clear problem
          statement about optimizing offer targeting by demographic groups,
          defined key metrics such as response rate and engagement rate, and
          justified these metrics in relation to the project's goals.
        </li>
        <li>
          <strong>Data Analysis and Exploration</strong>: We conducted an
          exploratory data analysis (EDA) to understand the relationships in the
          data. We explored demographics, offer details, and user interaction
          sequences, which allowed us to perform feature engineering, such as
          calculating <em>days_since_signup</em> and <em>spending_category</em>.
          This stage involved extensive visualizations to identify correlations
          between demographics, offer types, and engagement patterns.
        </li>
        <li>
          <strong>Preprocessing Pipeline</strong>: We merged the datasets to
          create a comprehensive dataset of user interactions, handled missing
          values, and encoded categorical variables like <em>offer_type</em> and
          <em>gender</em> using one-hot encoding. We also engineered additional
          features such as <em>total_spent_during_offer</em> and
          <em>viewed_offer_before_purchase</em>.
        </li>
        <li>
          <strong>Modeling</strong>: We trained initial models on preprocessed
          data to predict whether users would respond to an offer. We used a
          Random Forest Classifier, starting with a basic heuristic or
          rule-based model as a baseline, and iteratively refined it by adding
          more complex features and hyperparameter tuning. Alternative models
          such as logistic regression and gradient boosting were also tested and
          compared.
        </li>
      </ul>

      <h2>Discussion of the Expected Solution</h2>
      <p>
        The proposed solution integrates several components that work together
        to address the problem effectively:
      </p>
      <h3>Overall Architecture or Workflow</h3>
      <p>
        The project follows a well-defined workflow that integrates data
        preprocessing, feature engineering, modeling, and evaluation stages:
      </p>
      <ul>
        <li>
          <strong>Data Preprocessing</strong>: This step involved preparing and
          cleaning the data by handling missing values, encoding categorical
          variables, and engineering features. These preprocessed data were then
          merged to create a unified dataset that captured user, offer, and
          interaction details comprehensively.
        </li>
        <li>
          <strong>Modeling</strong>: We aimed to train a
          <strong>Random Forest Classifier</strong> model capable of predicting
          which demographic groups would be most responsive to each type of
          offer and the optimal channels to reach them. The model was chosen for
          its robustness in handling a mix of feature types and its capability
          to manage complex interactions between variables. Additionally, we
          used GridSearchCV for hyperparameter tuning to optimize model
          performance.
        </li>
        <li>
          <strong>Evaluation</strong>: Model evaluation was done using metrics
          like accuracy, precision, recall, and F1-score. These metrics helped
          us understand the model's effectiveness in predicting offer
          completions. The evaluation results were visualized using feature
          importance plots, confusion matrices, and ROC curves to interpret the
          model's behavior and identify areas for improvement.
        </li>
      </ul>
      <h2>Data Preprocessing</h2>
      <p>
        In this section, we outline the various steps taken to preprocess the
        data, ensuring it is clean, consistent, and ready for analysis. Each
        step includes a brief description and its intended purpose, making it
        easier to understand the transformation process involved.
      </p>

      <h3>1. Handle Missing Values, Duplicates, and Outliers</h3>
      <p>To ensure data quality, the following steps were taken:</p>
      <ul>
        <li>
          Checked for missing values across all datasets and filtered out
          problematic entries (e.g., rows with inconsistent values such as age
          118 with missing gender and income).
        </li>
        <li>
          Filtered out redundant or outlier records to improve data reliability.
        </li>
      </ul>

      <h3>2. Data Transformation and Feature Encoding</h3>
      <h4>Portfolio Dataset</h4>
      <ul>
        <li>
          Applied one-hot encoding to the <code>channels</code> column to
          represent the various marketing channels.
        </li>
        <li>
          Performed one-hot encoding on the <code>offer_type</code> column, and
          removed the <code>offer_type_informational</code> column to reduce
          redundancy.
        </li>
      </ul>

      <h4>Profile Dataset</h4>
      <ul>
        <li>
          Filtered rows where age was 118 and gender and income were missing, as
          they were identified as unreliable data.
        </li>
        <li>
          Converted the <code>became_member_on</code> column to datetime format
          for better date handling.
        </li>
        <li>
          Applied one-hot encoding to the <code>gender</code> column and dropped
          the <code>gender_O</code> column to avoid redundancy.
        </li>
        <li>
          Scaled the <code>income</code> column by dividing it by 1000 and
          renamed it to <code>income/k</code> for easier readability.
        </li>
      </ul>

      <h4>Transcript Dataset</h4>
      <ul>
        <li>
          Normalized the <code>value</code> column, which contained
          dictionaries, by expanding them into separate columns.
        </li>
        <li>
          Consolidated the <code>offer id</code> and
          <code>offer_id</code> columns into a single column named
          <code>offer_id</code>.
        </li>
        <li>
          Filled missing values in the <code>amount</code> and
          <code>reward</code> columns with 0, and filled
          <code>offer_id</code> with an empty string for missing entries.
        </li>
        <li>
          Applied one-hot encoding to the <code>event</code> column and dropped
          the <code>event_transaction</code> column to reduce redundancy.
        </li>
      </ul>

      <h3>3. Merging Datasets</h3>
      <p>To enable comprehensive analysis, all three datasets were merged:</p>
      <ul>
        <li>
          Profile and Transcript datasets were merged based on user IDs, and the
          redundant <code>id</code> column was dropped.
        </li>
        <li>
          The merged dataset was then combined with the Portfolio dataset using
          the <code>offer_id</code> key, and redundant columns were dropped.
        </li>
      </ul>

      <h3>4. Final Cleaning and Saving Processed Data</h3>
      <ul>
        <li>
          Filled missing values for demographic information and filtered out
          rows where such data was not available, as this information is crucial
          for identifying responsive user groups.
        </li>
        <li>
          Dropped unnecessary columns like <code>amount</code> and
          <code>reward</code> to maintain focus on relevant analysis attributes.
        </li>
        <li>
          Saved the fully processed dataset as a CSV file for further analysis
          and exploration.
        </li>
      </ul>

      <h3>Conclusion</h3>
      <p>
        By following these preprocessing steps, the data was thoroughly cleaned,
        transformed, and made ready for exploratory data analysis and model
        building. This process helps ensure that the dataset is of high quality
        and facilitates accurate analysis to achieve meaningful insights.
      </p>
      <h2>Metrics with Justification</h2>
      <p>
        In this section, we define the key metrics used for evaluating the
        effectiveness of the offer strategies and justify their selection. These
        metrics are used to assess user interactions with the offers and
        understand how various demographic groups respond to different types of
        offers.
      </p>

      <h2>1. Offer Funnel Metrics</h2>
      <h3>Goal</h3>
      <p>Measure how users interact with offers from start to completion.</p>
      <h3>Data Sources</h3>
      <ul>
        <li>
          <strong>transcript</strong>: For tracking offer events such as
          received, viewed, and completed.
        </li>
        <li>
          <strong>portfolio</strong>: To identify offer characteristics like
          type (BOGO, discount, informational).
        </li>
      </ul>
      <h3>Steps</h3>
      <p>
        Filter the <code>transcript</code> by <code>event</code> to identify
        "offer received," "offer viewed," and "offer completed" actions. For
        each offer in the <code>portfolio</code>, calculate:
      </p>
      <ul>
        <li>
          <strong>Received-to-View Rate</strong>: How many users viewed the
          offer after receiving it.
        </li>
        <li>
          <strong>View-to-Completion Rate</strong>: Of the offers viewed, how
          many were completed.
        </li>
        <li>
          <strong>Overall Completion Rate</strong>: The percentage of received
          offers that were eventually completed.
        </li>
      </ul>
      <h3>Key Findings</h3>
      <ul>
        <li>
          <strong>Received-to-View Rate</strong>: 74.98% - Approximately 75% of
          received offers were viewed by users. This suggests effective delivery
          channels that capture user attention.
        </li>
        <li>
          <strong>View-to-Completion Rate</strong>: 65.07% - About 65% of viewed
          offers were completed. This indicates reasonable offer design but also
          shows room for improvement.
        </li>
        <li>
          <strong>Overall Completion Rate</strong>: 48.79% - Less than half of
          all offers received were completed, highlighting opportunities for
          optimizing offer structure and incentives.
        </li>
      </ul>
      <h3>Insights</h3>
      <p>
        The high received-to-view rate suggests that initial marketing and
        communication strategies are effective. However, the lower completion
        rate indicates that more work is needed to improve the offer funnel,
        potentially by simplifying the requirements or increasing the perceived
        value of offers.
      </p>

      <h2>2. Demographic-Based Offer Engagement Rate</h2>
      <h3>Goal</h3>
      <p>
        Evaluate how users of different demographic segments engage with offers
        by identifying which age groups, income brackets, and genders are most
        likely to view or complete an offer after receiving it.
      </p>
      <h3>Data Sources</h3>
      <ul>
        <li>
          <strong>transcript</strong>: Includes event information related to
          offer activities (offer received, viewed, completed).
        </li>
        <li>
          <strong>profile</strong>: Contains demographic data such as age,
          gender, and income.
        </li>
      </ul>
      <h3>Steps</h3>
      <ul>
        <li>Group data by demographic variables:</li>
        <ul>
          <li><strong>Age Group</strong>: Group by age.</li>
          <li><strong>Income Group</strong>: Group by income brackets.</li>
          <li><strong>Gender Group</strong>: Group by gender.</li>
        </ul>
        <li>Calculate engagement metrics for each demographic group:</li>
        <ul>
          <li>
            <strong>Total Offers Received</strong>: Aggregate the number of
            offers received for each demographic.
          </li>
          <li>
            <strong>Total Offers Viewed</strong>: Aggregate the number of offers
            viewed for each demographic.
          </li>
          <li>
            <strong>Total Offers Completed</strong>: Aggregate the number of
            offers completed for each demographic.
          </li>
          <li>
            Calculate response rates for each group, such as
            <strong>View Rate</strong> (total viewed / total received) and
            <strong>Completion Rate</strong> (total completed / total received).
          </li>
        </ul>
      </ul>
      <h3>Key Findings and Insights</h3>
      <h4>Age Group Engagement</h4>
      <p>
        Older age groups tend to have higher view and completion rates compared
        to younger users. Targeting offers specifically to older demographics
        might result in higher completion rates.
      </p>
      <img
        src="Completion_Rates_by_age_offer_Type.png"
        alt="Completion Rates by Age and Offer Type"
      />

      <h4>Income Group Engagement</h4>
      <p>
        Higher-income individuals have higher view and completion rates,
        indicating a preference for offers that are more exclusive or provide
        higher rewards.
      </p>
      <img
        src="Completion_Rates_by_income_offer_Type.png"
        alt="Completion Rates by Income and Offer Type"
      />

      <h4>Gender Group Engagement</h4>
      <p>
        Engagement rates are relatively consistent across genders, suggesting
        that gender-neutral campaigns are effective.
      </p>
      <img
        src="Completion_Rates_by_gender_offer_Type.png"
        alt="Completion Rates by Gender and Offer Type"
      />

      <h2>3. Response Rates by Demographics</h2>
      <h3>Goal</h3>
      <p>
        Understand which demographic groups respond best to each offer type to
        tailor offers accordingly.
      </p>
      <h3>Data Sources</h3>
      <ul>
        <li>
          <strong>profile</strong>: For demographic data, including age, income,
          and gender.
        </li>
        <li>
          <strong>transcript</strong>: For tracking offer responses by user.
        </li>
        <li>
          <strong>portfolio</strong>: For offer characteristics to map responses
          to specific offer types.
        </li>
      </ul>
      <h3>Steps</h3>
      <ul>
        <li>
          Merge <code>profile</code> and <code>transcript</code> datasets to
          create demographic segments.
        </li>
        <li>
          Calculate view and completion rates for each segment and compare
          across different offer types.
        </li>
      </ul>
      <h3>Key Findings and Insights</h3>
      <h4>Age Group Response</h4>
      <p>
        Adults and seniors are most engaged with offers, especially discount
        offers. Designing targeted campaigns for these age groups with
        discount-based incentives could enhance completion rates further.
      </p>
      <img
        src="View_Rates_by_age_offer_Type.png"
        alt="View Rates by Age and Offer Type"
      />

      <h4>Income Group Response</h4>
      <p>
        Higher-income individuals are more responsive to discount offers,
        suggesting that offers with greater monetary rewards can maximize
        engagement in this group.
      </p>
      <img
        src="View_Rates_by_income_offer_Type.png"
        alt="View Rates by Income and Offer Type"
      />

      <h4>Gender Group Response</h4>
      <p>
        There is no significant difference in gender-based engagement. However,
        the "Other" gender group shows slightly higher engagement rates,
        presenting an opportunity for more personalized marketing campaigns.
      </p>
      <img
        src="View_Rates_by_gender_offer_Type.png"
        alt="View Rates by Gender and Offer Type"
      />
      <h2>Exploratory Data Analysis (EDA)</h2>
      <p>
        In this section, we conduct an exploratory data analysis to document the
        key findings from the data, highlighting notable patterns, trends, and
        insights gained through visualizations and statistical summaries.
      </p>

      <h3>1. Distribution of Channels Used for Delivering Offers</h3>
      <img
        src="Channels_distribution.png"
        alt="Distribution of Channels Used for Delivering Offers"
        style="width: 100%; height: auto"
      />
      <h4>Observations</h4>
      <ul>
        <li>
          <strong>Email</strong> appears to be the most commonly used channel
          for delivering offers, followed by <strong>mobile</strong>.
        </li>
        <li>
          <strong>Social</strong> has the lowest frequency compared to other
          channels.
        </li>
        <li>
          The <strong>web</strong> channel also has a significant number of
          offers delivered but falls below <strong>email</strong> and
          <strong>mobile</strong>.
        </li>
      </ul>
      <h4>Insights</h4>
      <ul>
        <li>
          <strong>Email</strong> and <strong>mobile</strong> are the preferred
          communication methods for delivering offers, possibly because they are
          more direct and provide higher customer reach.
        </li>
        <li>
          <strong>Social</strong> media is less frequently used, suggesting that
          customers might be less responsive to this channel or that Starbucks
          emphasizes more direct communication channels.
        </li>
      </ul>

      <h3>2. Distribution of Offer Types</h3>
      <img
        src="Offer_types_distribution.png"
        alt="Distribution of Offer Types"
        style="width: 100%; height: auto"
      />
      <h4>Observations</h4>
      <ul>
        <li>
          <strong>BOGO</strong> and <strong>Discount</strong> offers are almost
          equally distributed, with both having significantly higher frequencies
          compared to <strong>Informational</strong> offers.
        </li>
        <li>
          <strong>Informational</strong> offers are the least frequently
          distributed.
        </li>
      </ul>
      <h4>Insights</h4>
      <ul>
        <li>
          The high frequency of <strong>BOGO</strong> and
          <strong>Discount</strong> offers suggests that Starbucks relies
          heavily on incentivizing customers to make purchases through monetary
          rewards.
        </li>
        <li>
          <strong>Informational</strong> offers are less common, which might
          indicate they are used for brand awareness or general product
          information rather than direct sales.
        </li>
        <li>
          The near-equal distribution between <strong>BOGO</strong> and
          <strong>Discount</strong> suggests that Starbucks is likely testing
          both types to see which generates higher engagement or sales.
        </li>
      </ul>

      <h3>3. Distribution of Income (in thousands)</h3>
      <img
        src="Income_distribution.png"
        alt="Distribution of Income (in thousands)"
        style="width: 100%; height: auto"
      />
      <h4>Observations</h4>
      <ul>
        <li>
          The <strong>income</strong> distribution appears to be skewed towards
          the lower end, with most customers falling between
          <strong>30k</strong> and <strong>80k</strong>.
        </li>
        <li>
          The peak income range seems to be around <strong>50k-60k</strong>.
        </li>
        <li>
          There is a gradual decline in frequency as income increases beyond
          <strong>80k</strong>.
        </li>
      </ul>
      <h4>Insights</h4>
      <ul>
        <li>
          The skewed distribution towards lower income levels might indicate
          that the target demographic for Starbucks offers is primarily
          middle-income customers.
        </li>
        <li>
          Since the majority of customers fall between <strong>30k</strong> and
          <strong>80k</strong>, it could be beneficial to tailor offers
          specifically for this income range, focusing on incentives that
          resonate with the spending habits of this group.
        </li>
        <li>
          The decline beyond <strong>80k</strong> suggests that higher-income
          customers may be less interested in Starbucks offers or simply
          constitute a smaller portion of the target audience.
        </li>
      </ul>

      <h3>Summary</h3>
      <ul>
        <li>
          <strong>Channels</strong>: Email and mobile are the most utilized
          channels, suggesting they are more effective or preferred by
          customers.
        </li>
        <li>
          <strong>Offer Types</strong>: BOGO and Discount offers dominate,
          indicating a focus on driving sales through direct incentives rather
          than purely informational campaigns.
        </li>
        <li>
          <strong>Income Distribution</strong>: The majority of customers are in
          the middle-income range, providing an opportunity to optimize offers
          for this demographic to maximize engagement and spending.
        </li>
      </ul>

      <h2>Modeling</h2>
      <p>
        In this section, we present the details of the model used in this
        project, providing an overview of the underlying algorithms, specific
        considerations, and modifications that were applied during the modeling
        process. This section includes relevant code snippets to illustrate the
        approach and methodology.
      </p>

      <h3>Purpose</h3>
      <p>
        The purpose of this section is to develop predictive models or
        analytical frameworks to address project goals, utilizing appropriate
        modeling techniques, and evaluating their performance.
      </p>

      <h3>1. Model Selection and Preprocessing</h3>
      <p>
        To achieve our objectives, we opted for a classification model,
        specifically using a <strong>Random Forest Classifier</strong> due to
        its effectiveness in handling categorical data, feature importance, and
        robust performance in classification tasks.
      </p>
      <ul>
        <li><strong>Model Type:</strong> Random Forest Classifier</li>
        <li>
          <strong>Features:</strong> Demographic information, offer type,
          channels, etc.
        </li>
        <li>
          <strong>Target:</strong> Whether the customer completed an offer
          (<code>event_offer completed</code>).
        </li>
      </ul>

      <h3>2. Data Preparation</h3>
      <p>The following data preparation steps were taken before modeling:</p>
      <ul>
        <li>
          Dropped unnecessary columns such as <code>person</code>,
          <code>time</code>, and <code>offer_id</code> that were not
          contributing to the predictive analysis.
        </li>
        <li>
          Categorized the <code>age</code> and <code>income</code> features into
          distinct ranges, providing more interpretability and generalization.
        </li>
        <li>
          Performed one-hot encoding on the categorical columns
          <code>age_category</code> and <code>income_category</code> to convert
          them into numerical features.
        </li>
        <li>
          Converted boolean and float columns into integer types for
          compatibility with the Random Forest Classifier.
        </li>
      </ul>

      <h3>3. Model Training and Evaluation</h3>
      <p>Once the data was prepared, we proceeded with training the model:</p>
      <ul>
        <li>
          Defined the features (<code>X</code>) and target variable
          (<code>y</code>).
        </li>
        <li>
          Split the dataset into training and testing sets, using an 80-20 split
          ratio.
        </li>
        <li>
          Defined a pipeline consisting of a <code>StandardScaler</code> for
          feature scaling and the <code>RandomForestClassifier</code>.
        </li>
      </ul>
      <h3>4. Model Performance</h3>
      <p>
        The model was evaluated using metrics such as
        <strong>accuracy</strong> and a
        <strong>classification report</strong> that includes precision, recall,
        and F1-score. The accuracy score obtained for the test set was
        <code>1.00</code>.
      </p>
      <pre><code>Classification Report:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     23285
           1       1.00      1.00      1.00      6476

    accuracy                           1.00     29761
   macro avg       1.00      1.00      1.00     29761
weighted avg       1.00      1.00      1.00     29761</code></pre>
      <p>
        These evaluation metrics provide a comprehensive view of how well the
        model performs in predicting whether a customer completed an offer based
        on their demographic data and the type of offer received.
      </p>
      <p>
        <strong
          >However, the perfect accuracy score of <code>1.00</code> indicates
          that the model is likely overfitting. Overfitting occurs when the
          model learns the training data too well, including noise and minor
          details, which leads to poor generalization on unseen data. In this
          case, the model might have memorized the training data, resulting in
          overly optimistic evaluation metrics on the test set.</strong
        >
      </p>
      <p>Potential reasons for overfitting in this model include:</p>
      <ul>
        <li>
          Using too many features without adequate feature selection or
          reduction, which can cause the model to pick up on irrelevant
          patterns.
        </li>
        <li>
          Insufficient regularization of the Random Forest Classifier, leading
          to high model complexity.
        </li>
      </ul>
      <p>
        To address overfitting, steps such as hyperparameter tuning, feature
        selection, or trying simpler models can be taken to improve the
        generalizability of the model.
      </p>
      <h2>Hyperparameter Tuning</h2>
      <p>
        In this section, we describe the process of hyperparameter tuning for
        the selected Random Forest model. Hyperparameter tuning is crucial for
        improving the performance of the model and preventing overfitting or
        underfitting. The tuning process allows us to find the best combination
        of parameters to achieve optimal results.
      </p>

      <h3>Purpose</h3>
      <p>
        The purpose of hyperparameter tuning is to improve model performance and
        prevent overfitting by finding the ideal combination of hyperparameter
        values for the Random Forest Classifier. This ensures that the model
        generalizes well to new, unseen data.
      </p>

      <h3>1. Hyperparameter Tuning Process</h3>
      <p>
        We used <strong>GridSearchCV</strong> to perform hyperparameter tuning.
        Grid search is an exhaustive search over specified parameter values,
        using cross-validation to evaluate model performance. Below, we outline
        the parameters that were tuned and the rationale behind each selection:
      </p>
      <ul>
        <li>
          <strong>n_estimators</strong>: The number of trees in the forest. We
          chose values of <code>3, 5, 10</code> to keep the model simple and
          prevent overfitting.
        </li>
        <li>
          <strong>max_depth</strong>: The maximum depth of the tree. Limiting
          tree depth helps reduce overfitting by restricting how complex the
          model can become. Values used were <code>3, 4, 5</code>.
        </li>
        <li>
          <strong>min_samples_split</strong>: The minimum number of samples
          required to split an internal node. Larger values prevent the model
          from learning overly specific patterns, thereby reducing overfitting.
          Values used were <code>3, 4, 5, 7</code>.
        </li>
        <li>
          <strong>min_samples_leaf</strong>: The minimum number of samples
          required to be at a leaf node. This helps create more generalized leaf
          nodes. Values used were <code>3, 4, 5, 7</code>.
        </li>
        <li>
          <strong>max_features</strong>: The number of features to consider when
          looking for the best split. We used <code>"sqrt"</code> and
          <code>"log2"</code> to limit the number of features considered at each
          split, helping prevent overfitting.
        </li>
      </ul>

      <h3>2. Grid Search Implementation</h3>
      <p>
        We used <code>GridSearchCV</code> from scikit-learn to perform the
        tuning.
      </p>
      <h3>3. Results and Insights</h3>
      <p>
        After tuning the hyperparameters, the best parameters were obtained
        based on cross-validation scores. The results showed that:
      </p>
      <pre><code>Best Parameters: <br>
        {'classifier__max_depth': 5,<br> 
        'classifier__max_features': 'sqrt',<br>
        'classifier__min_samples_leaf': 3,<br> 
        'classifier__min_samples_split': 3,<br> 
        'classifier__n_estimators': 10}<br>
        Best Cross-Validation Score: 0.86
      </code></pre>
      <p>
        The hyperparameter tuning process aimed to reduce the model's complexity
        and improve generalization. The selected parameters helped prevent
        overfitting by limiting the depth of the trees, reducing the number of
        features considered, and ensuring minimum samples at leaf nodes and
        internal splits.
      </p>

      <h3>4. Model Evaluation</h3>
      <p>
        After applying the best hyperparameters, the tuned model was evaluated
        on the test set:
      </p>
      <pre><code>from sklearn.metrics import accuracy_score, classification_report

# Make predictions on the test set
y_pred_gs = grid_search.predict(X_test)

# Step 6: Evaluate the model
print(f"Accuracy: {accuracy_score(y_test, y_pred_gs):.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred_gs))</code></pre>
      <p>
        The accuracy of the tuned model was <code>0.91</code>, which, while
        lower than the previous overfitted model, provides a more realistic and
        generalizable performance evaluation. Below is the classification
        report:
      </p>
      <pre><code>Classification Report:
              precision    recall  f1-score   support

           0       0.90      1.00      0.95     23285
           1       1.00      0.59      0.74      6476

    accuracy                           0.91     29761
   macro avg       0.95      0.80      0.84     29761
weighted avg       0.92      0.91      0.90     29761</code></pre>
      <p>
        Although the accuracy decreased compared to the overfitted model, the
        generalization of the model improved significantly, which is critical
        for real-world predictive performance.
      </p>
      <h2>Results</h2>
      <p>
        In this section, we present the results of the model evaluation and
        performance, including relevant metrics, visualizations, and other
        outputs that demonstrate the effectiveness of the solution. The results
        are interpreted to highlight key insights or observations that were
        derived from the model's predictions.
      </p>

      <h3>1. Confusion Matrix</h3>
      <p>
        The confusion matrix provides a visual representation of the model's
        predictions compared to the actual values:
      </p>
      <img
        src="confusion_matrix.png"
        alt="Confusion Matrix for Random Forest Classifier"
        style="width: 100%; height: auto"
      />
      <p>
        The confusion matrix highlights that the model correctly predicts most
        of the non-completions, but there are some false negatives where the
        model fails to predict the completion of offers.
      </p>

      <h3>2. Feature Importance</h3>
      <p>
        The feature importance plot provides insights into which features were
        most significant in predicting offer completions:
      </p>
      <img
        src="feature_importance.png"
        alt="Feature Importances for Random Forest Classifier"
        style="width: 100%; height: auto"
      />
      <p>
        From the feature importance plot, it is evident that the features
        <strong>event_offer received</strong> and
        <strong>event_offer viewed</strong> had the most influence on the
        predictions. Other features such as <strong>offer_type_bogo</strong> and
        <strong>offer_type_discount</strong> also played a role, indicating that
        the type of offer impacts customer behavior.
      </p>

      <h3>3. ROC Curve</h3>
      <p>
        The Receiver Operating Characteristic (ROC) curve is used to visualize
        the trade-off between sensitivity and specificity:
      </p>
      <img
        src="roc_curve.png"
        alt="ROC Curve for Random Forest Classifier"
        style="width: 100%; height: auto"
      />
      <p>
        The ROC curve shows that the model performs well overall, with an Area
        Under the Curve (AUC) value of <strong>1.00</strong>, indicating a high
        discriminative ability. However, this might also suggest a potential
        overfitting issue, as a perfect AUC score is often a sign of a model
        that has learned the training data too well.
      </p>

      <h3>4. Comparison Table</h3>
      <p>
        The table below compares the performance of different model variations
        or hyperparameter settings:
      </p>
      <table border="1" style="width: 100%; text-align: center">
        <thead>
          <tr>
            <th>Model Variation</th>
            <th>Accuracy</th>
            <th>Precision (Class 1)</th>
            <th>Recall (Class 1)</th>
            <th>F1-Score (Class 1)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Initial Random Forest (No Tuning)</td>
            <td>1.00</td>
            <td>1.00</td>
            <td>1.00</td>
            <td>1.00</td>
          </tr>
          <tr>
            <td>Tuned Random Forest</td>
            <td>0.91</td>
            <td>1.00</td>
            <td>0.59</td>
            <td>0.74</td>
          </tr>
        </tbody>
      </table>
      <p>
        The comparison table shows that the initial model had perfect metrics,
        suggesting overfitting, whereas the tuned model achieved a more
        realistic and generalizable performance.
      </p>

      <h3>5. Conclusion</h3>
      <p>
        In conclusion, the Random Forest model with tuned hyperparameters
        provided a more reliable and realistic assessment of customer behavior
        compared to the overfitted model. While the accuracy of
        <strong>0.91</strong> is strong, there is still room for improvement,
        particularly in identifying customers who complete the offers. Future
        steps could involve further tuning of the model, trying alternative
        algorithms like <strong>Gradient Boosting</strong> or
        <strong>XGBoost</strong>, and incorporating additional features that
        capture customer engagement patterns.
      </p>
      <!DOCTYPE html>
      <html lang="en">
        <head>
          <meta charset="UTF-8" />
          <meta
            name="viewport"
            content="width=device-width, initial-scale=1.0"
          />
          <title>Improvements and Acknowledgment</title>
        </head>
        <body>
          <h2>Improvements</h2>
          <p>
            In this section, we identify the limitations, challenges, and areas
            for improvement in the project. We also discuss potential
            enhancements or future directions that could further enhance the
            solution or address any remaining gaps.
          </p>

          <h3>Identified Challenges and Areas for Improvement</h3>
          <ul>
            <li>
              <strong>Demographic Factors:</strong> The analysis revealed that
              demographic factors such as age, gender, and income have limited
              influence on offer completion. To address this, new features
              related to user engagement could be developed.
            </li>
            <li>
              <strong>Class Imbalance:</strong> The model showed high recall for
              class 0 (offers not completed) but struggled with recall for class
              1 (offers completed). This indicates a potential class imbalance
              issue, which makes it challenging for the model to correctly
              identify all potential positive cases.
            </li>
            <li>
              <strong>Offer Response Attribution:</strong> Determining why a
              user completed an offer remains challenging due to the complex
              interplay of offer type, channel, timing, and user behavior.
              Additional behavioral features could provide more insights into
              the completion of offers.
            </li>
          </ul>

          <h3>Improvement Ideas</h3>
          <ul>
            <li>
              <strong>Feature Engineering:</strong>
              <ul>
                <li>
                  Develop user engagement features that capture user behavior,
                  such as time taken to view an offer, number of interactions
                  before offer completion, or frequency of offer interactions.
                  These features could provide more insight into user engagement
                  levels.
                </li>
                <li>
                  Integrate location data to identify regional differences in
                  offer responsiveness. Additionally, considering seasonal or
                  time-based factors might help account for fluctuations in user
                  behavior during different times of the year (e.g., holidays,
                  weekends).
                </li>
              </ul>
            </li>
            <li>
              <strong>Class Imbalance Handling:</strong>
              <ul>
                <li>
                  Apply techniques like <strong>SMOTE</strong> (Synthetic
                  Minority Over-sampling Technique) to balance the dataset and
                  improve model performance for minority classes.
                </li>
                <li>
                  Adjust class weights in the model to handle the imbalance more
                  effectively.
                </li>
              </ul>
            </li>
            <li>
              <strong>Channel-Specific Strategies:</strong>
              <ul>
                <li>
                  Given that web and mobile channels performed better in driving
                  offer completions, focusing on personalizing offers delivered
                  through these channels might improve the overall offer
                  completion rate.
                </li>
              </ul>
            </li>
          </ul>

          <h2>Acknowledgment</h2>
          <p>
            We would like to express our sincere gratitude to everyone who
            contributed to the success of this project:
          </p>
          <ul>
            <li>
              <strong>Mentors and Advisors:</strong> A special thanks to our
              mentors for their invaluable guidance throughout the project.
            </li>
            <li>
              <strong>Data Providers:</strong> We are grateful to the
              organizations and individuals who made the data available for
              analysis.
            </li>
            <li>
              <strong>Supportive Colleagues:</strong> We acknowledge the support
              of our colleagues, who provided feedback, shared resources, and
              assisted in the project implementation.
            </li>
            <li>
              <strong>Community Resources:</strong> We also appreciate the
              online communities such as Stack Overflow and data science forums
              that helped solve technical challenges during the course of this
              project.
            </li>
          </ul>
          <img src="happy_costumer.jpg" alt="happy_costumer">

          <h2>Summary of Findings</h2>
          <h3>1. Demographics and Channels Responsiveness</h3>
          <p>
            The analysis showed that demographic factors such as age, gender,
            and income have limited influence on offer completion. Channels like
            web and mobile showed higher importance in the model's predictions,
            indicating that users who received offers through these channels
            were more likely to respond positively.
          </p>
          <p>
            Offer types like BOGO and Discount contributed moderately to model
            predictions, while informational offers had lower completion rates,
            highlighting the need to focus more on incentivizing offers.
          </p>

          <h3>2. Challenges</h3>
          <p>
            The model faced challenges such as class imbalance, where it
            struggled to correctly identify all positive cases. Offer response
            attribution also remained complex due to the interplay of multiple
            factors.
          </p>

          <h3>3. Improvement Ideas</h3>
          <ul>
            <li>Feature engineering to capture user engagement behavior.</li>
            <li>
              Handling class imbalance using techniques like SMOTE or adjusting
              class weights.
            </li>
            <li>
              Channel-specific strategies to personalize offers through more
              effective channels like web and mobile.
            </li>
          </ul>
        </body>
      </html>

      <h2>References</h2>
      <ul>
        <li>
          <a href="#ref1"
            >[1] Scikit-Learn Documentation for Random Forest Classifier</a
          >
        </li>
        <li>
          <a href="#ref2"
            >[2] Article discussing class imbalance solutions in machine
            learning</a
          >
        </li>
        <li>
          <a href="#ref3"
            >[3] Source discussing the impact of different marketing channels on
            user behavior</a
          >
        </li>
      </ul>
    </div>
  </body>
</html>
